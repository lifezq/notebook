深入浅出搜索架构引擎、方案与细节（中）
=============

## 一、缘起            
[《深入浅出搜索架构（上篇）》](http://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=2651959895&idx=1&sn=de25ce2544c088ff9be0b93fd3ea4d15&chksm=bd2d078b8a5a8e9d5ae4339a683d3f980ff2994f3c10c4081c7bab7f0d77f37521de95e974bf&scene=21#wechat_redirect)详细介绍了：           
（1）全网搜索引擎架构与流程           
（2）站内搜索引擎架构与流程           
（3）搜索原理与核心数据结构           
 
本文重点介绍：           
（4）流量数据量由小到大，常见搜索方案与架构变迁           
（5）数据量、并发量、扩展性方案           
 
只要业务有检索需求，本文一定对你有帮助。           
 
## 二、检索需求的满足与架构演进           
任何互联网需求，或多或少有检索需求，还是以58同城的帖子业务场景为例，帖子的标题，帖子的内容有很强的用户检索需求，在业务、流量、并发量逐步递增的各个阶段，应该如何实现检索需求呢？           
 
原始阶段-LIKE           
数据在数据库中可能是这么存储的：           
t_tiezi(tid, title, content)           
满足标题、内容的检索需求可以通过LIKE实现：           
select tid from t_tiezi where content like ‘%天通苑%’           

能够快速满足业务需求，存在的问题也显而易见：           
（1）效率低，每次需要全表扫描，计算量大，并发高时cpu容易100%           
（2）不支持分词           
 
初级阶段-全文索引           
如何快速提高效率，支持分词，并对原有系统架构影响尽可能小呢，第一时间想到的是建立全文索引：           
alter table t_tiezi add fulltext(title,content)           
使用match和against实现索引字段上的查询需求。           

全文索引能够快速实现业务上分词的需求，并且快速提升性能（分词后倒排，至少不要全表扫描了），但也存在一些问题：           
（1）只适用于MyISAM           
（2）由于全文索引利用的是数据库特性，搜索需求和普通CURD需求耦合在数据库中：检索需求并发大时，可能影响CURD的请求；CURD并发大时，检索会非常的慢；           
（3）数据量达到百万级别，性能还是会显著降低，查询返回时间很长，业务难以接受           
（4）比较难水平扩展           
 
中级阶段-开源外置索引           
为了解决全文索的局限性，当数据量增加到大几百万，千万级别时，就要考虑外置索引了。外置索引的核心思路是：索引数据与原始数据分离，前者满足搜索需求，后者满足CURD需求，通过一定的机制（双写，通知，定期重建）来保证数据的一致性。           

原始数据可以继续使用Mysql来存储，外置索引如何实施？Solr，Lucene，ES都是常见的开源方案。           
楼主强烈推荐ES（ElasticSearch），原因是Lucene虽好，但始终有一些不足：           
（1）Lucene只是一个库，潜台词是，需要自己做服务，自己实现高可用/可扩展/负载均衡等复杂特性           
（2）Lucene只支持Java，如果要支持其他语言，还是得自己做服务           
（3）Lucene不友好，这是很致命的，非常复杂，使用者往往需要深入了解搜索的知识来理解它的工作原理，为了屏蔽其复杂性，一个办法是自己做服务           
…           
…           

为了改善Lucene的各项不足，解决方案都是“封装一个接口友好的服务，屏蔽底层复杂性”，于是有了ES：           
（1）ES是一个以Lucene为内核来实现搜索功能，提供REStful接口的服务           
（2）ES能够支持很大数据量的信息存储，支持很高并发的搜索请求           
（3）ES支持集群，向使用者屏蔽高可用/可扩展/负载均衡等复杂特性           
 
目前58到家使用ES作为核心，实现了自己的搜索服务平台，能够通过在平台上简单的配置，实现业务方的搜索需求。           
搜索服务数据量最大的“接口耗时数据收集”需求，数据量大概在7亿左右；并发量最大的“经纬度，地理位置搜索”需求，线上平均并发量大概在600左右，压测数据并发量在6000左右。           
 
结论：ES完全能满足10亿数据量，5k吞吐量的常见搜索业务需求，强烈推荐。           
 
高级阶段-自研搜索引擎           
当数据量进一步增加，达到10亿、100亿数据量；并发量也进一步增加，达到每秒10万吞吐；业务个性也逐步增加的时候，就需要自研搜索引擎了，定制化实现搜索内核了。           

## 三、数据量、并发量、扩展性方案           
到了定制化自研搜索引擎的阶段，超大数据量、超高并发量为设计重点，为了达到“无限容量、无限并发”的需求，架构设计需要重点考虑“扩展性”，力争做到：增加机器就能扩容（数据量+并发量）。           

58同城的自研搜索引擎E-search初步架构图如下：           

![img01](http://mmbiz.qpic.cn/mmbiz_png/YrezxckhYOzQvxZXgOHJnOSicN3LWTAnHq0FdfZ6ROpzMH02xoxkdWk2xBpNUjtAd4yMNibmarJrV5s4Eb5icIF8g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1)        

（1）上层proxy（粉色）是接入集群，为对外门户，接受搜索请求，其无状态性能够保证增加机器就能扩充proxy集群性能           
（2）中层merger（浅蓝色）是逻辑集群，主要用于实现搜索合并，以及打分排序，业务相关的rank就在这一层实现，其无状态性也能够保证增加机器就能扩充merger集群性能                      
（3）底层searcher（暗红色大框）是检索集群，服务和索引数据部署在同一台机器上，服务启动时可以加载索引数据到内存，请求访问时从内存中load数据，访问速度很快                      
（3.1）为了满足数据容量的扩展性，索引数据进行了水平切分，增加切分份数，就能够无限扩展性能，如上图searcher分为了4组           
（3.2）为了满足一份数据的性能扩展性，同一份数据进行了冗余，理论上做到增加机器就无限扩展性能，如上图每组searcher又冗余了2份           

如此设计，真正做到做到增加机器就能承载更多的数据量，响应更高的并发量。           
 
## 四、总结           
为了满足搜索业务的需求，随着数据量和并发量的增长，搜索架构一般会经历这么几个阶段：           
（1）原始阶段-LIKE           
（2）初级阶段-全文索引           
（3）中级阶段-开源外置索引           
（4）高级阶段-自研搜索引擎           

你的搜索架构到了哪一个阶段？数据量、并发量、好的经验欢迎分享？           

欢迎留言，有问必答。           
如果有收获，欢迎帮转。           

## 五、下章预告           
实时搜索引擎核心技术，站长发布1个新网页，Google如何做到15分钟后检索出来。            
==【（中）完】==           

[阅读原文](http://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=2651959917&idx=1&sn=8faeae7419a756b0c355af2b30c255df&chksm=bd2d07b18a5a8ea75f16f7e98ea897c7e7f47a0441c64bdaef8445a2100e0bdd2a7de99786c0&scene=21#wechat_redirect)    

相关文章：           
[如何快速实现高并发短文检索](http://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=2651959451&idx=1&sn=991d9c3737d7db50a8351d50cdf6419d&scene=21#wechat_redirect)            
[深入浅出搜索引擎架构、方案、细节](http://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=2651959895&idx=1&sn=de25ce2544c088ff9be0b93fd3ea4d15&chksm=bd2d078b8a5a8e9d5ae4339a683d3f980ff2994f3c10c4081c7bab7f0d77f37521de95e974bf&scene=21#wechat_redirect)           
